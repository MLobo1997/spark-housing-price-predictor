{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006b4137-6cc5-40d0-8571-15555b7192a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1c871-e3c7-47d9-9593-b044f1a60c68",
   "metadata": {},
   "source": [
    "# Feature engineering/ Pipeline building\n",
    "The purpose of this notebook is to walk through the steps of each created feature and of building the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2bba2-bd28-4b81-a152-a1e99343baec",
   "metadata": {},
   "source": [
    "# Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65062519-b574-48f0-a1cf-163e28035102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4448b6-1a37-48bf-a604-2f81929b7a3e",
   "metadata": {},
   "source": [
    "# Defining a schema and loading the data\n",
    "* You can check the input schema definition in [house_price_predictor/schema.py](house_price_predictor/schema.py).\n",
    "* I prefer to use Doubles for all features instead of using Integers for some features, because it's more flexible and avoids the pipeline erroring out in case of noise.\n",
    "* Even though waterfront could be boolean I will treat it as numerical because it's easier for imputation\n",
    "* The same with all other categorical features (e.g., such as condition or grade), because it was clear from the data visualization that their order is a predictor of value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a00d646-5501-4697-b994-fdc2475d74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from house_price_predictor.schema import schema\n",
    "df = spark.read.csv(\"data/kc_house_data.csv\", header=True, schema=schema, mode=\"PERMISSIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3a2cc-aa8d-4e5c-a54d-9595274a8732",
   "metadata": {},
   "source": [
    "# Creating new features\n",
    "* For the source code check the files [house_price_predictor/features/sql_transformers.py](house_price_predictor/features/sql_transformers.py) and [house_price_predictor/features/zipcode_ranker.py](house_price_predictor/features/zipcode_ranker.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e33ef-3e94-477d-be2d-003dbf5cd796",
   "metadata": {},
   "source": [
    "## Convert date\n",
    "* SQLTransformers are the most straightforward way of creating a serializable Spark Pipelines, so I will be using as much as I can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0296328c-cf64-49c1-ac29-90d1a99334d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|           date|     converted_date|\n",
      "+---------------+-------------------+\n",
      "|20141013T000000|2014-10-13 00:00:00|\n",
      "|20141209T000000|2014-12-09 00:00:00|\n",
      "|20150225T000000|2015-02-25 00:00:00|\n",
      "|20141209T000000|2014-12-09 00:00:00|\n",
      "|20150218T000000|2015-02-18 00:00:00|\n",
      "+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from house_price_predictor.features.sql_transformers import *\n",
    "\n",
    "colname, transformer = date_converter()\n",
    "df = transformer.transform(df)\n",
    "df.select(\"date\", colname).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c29db-7ca8-4a49-b579-0709a030428f",
   "metadata": {},
   "source": [
    "## Age of construction feature\n",
    "* This one probably doesn't bring much value, but it's useful to compare with `renovation_age` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ea3643-3049-4d8e-9270-4bd616cba181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------------+\n",
      "|           date|yr_built|construction_age|\n",
      "+---------------+--------+----------------+\n",
      "|20141013T000000|    1955|              59|\n",
      "|20141209T000000|    1951|              63|\n",
      "|20150225T000000|    1933|              82|\n",
      "|20141209T000000|    1965|              49|\n",
      "|20150218T000000|    1987|              28|\n",
      "+---------------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colname, transformer = construction_age_creator()\n",
    "df = transformer.transform(df)\n",
    "df.select(\"date\", \"yr_built\", colname).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608b4d0-8ea5-497f-b542-4aed30f18a39",
   "metadata": {},
   "source": [
    "## Age of renovation\n",
    "* As mentioned in the [data viz notebook](00_data_viz.ipynb), the `yr_renovated` is imputed with 0's, so I am creating a `renovation_age` feature to substitute it.\n",
    "* When creating a renovation age feature, it makes sense that its maximum value (in case `yr_renovated == 0`) is the construction age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2678a22-30bd-4844-b0d9-efeee7b00f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------------+----------------+--------------+\n",
      "|           date|yr_built|yr_renovated|construction_age|renovation_age|\n",
      "+---------------+--------+------------+----------------+--------------+\n",
      "|20141013T000000|    1955|           0|              59|            59|\n",
      "|20141209T000000|    1951|        1991|              63|            23|\n",
      "|20150225T000000|    1933|           0|              82|            82|\n",
      "|20141209T000000|    1965|           0|              49|            49|\n",
      "|20150218T000000|    1987|           0|              28|            28|\n",
      "+---------------+--------+------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+--------+------------+----------------+--------------+\n",
      "|           date|yr_built|yr_renovated|construction_age|renovation_age|\n",
      "+---------------+--------+------------+----------------+--------------+\n",
      "|20141209T000000|    1951|        1991|              63|            23|\n",
      "|20140613T000000|    1930|        2002|              84|            12|\n",
      "|20140908T000000|    1946|        1991|              68|            23|\n",
      "|20141007T000000|    1950|        2010|              64|             4|\n",
      "|20141121T000000|    1900|        1999|             114|            15|\n",
      "+---------------+--------+------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colname, transformer = renovation_age_creator()\n",
    "df = transformer.transform(df)\n",
    "df.select(\"date\", \"yr_built\", \"yr_renovated\", \"construction_age\", colname).show(5)\n",
    "df.select(\"date\", \"yr_built\", \"yr_renovated\", \"construction_age\", colname).where(col(\"yr_renovated\") != 0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4398833-e470-4ad0-ad9f-812598be5570",
   "metadata": {},
   "source": [
    "## Zipcode\n",
    "\n",
    "* The`zipcode_price_rank` feature represents the position of a given zipcode in the rank of most expensive, on average, zipcodes (being 0 the most expensive zipcode). \n",
    "* You can check the code in this file [house_price_predictor/features/zipcode_ranker.py](house_price_predictor/features/zipcode_ranker.py)\n",
    "* One concern here is label leakage. But I will be ensuring this is only fitted with the train dataset by using a Pipeline, so no problem.\n",
    "* This approach is much lighter than OneHotEncoding of the zipcodes.\n",
    "* Using the SQLTransformer was not an option because it would require SQL UDFs and these cannot be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b46608-09a4-4f02-8bf9-734efd245eec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|zipcode|zipcode_price_rank|\n",
      "+-------+------------------+\n",
      "|  98178|                57|\n",
      "|  98125|                37|\n",
      "|  98028|                39|\n",
      "|  98136|                31|\n",
      "|  98074|                13|\n",
      "|  98053|                15|\n",
      "|  98003|                62|\n",
      "|  98198|                59|\n",
      "|  98146|                50|\n",
      "|  98038|                49|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from house_price_predictor.features.zipcode_ranker import ZipcodeRanker\n",
    "zipcode_ranker = ZipcodeRanker().fit(df)\n",
    "df = zipcode_ranker.transform(df)\n",
    "df.select(\"zipcode\", \"zipcode_price_rank\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dd79d-4434-46ed-b02f-c6166991b4f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building the pipeline\n",
    "* See the source code in [house_price_predictor/features/pipeline.py](house_price_predictor/features/pipeline.py)\n",
    "* Using an Imputer with the mean values because these should impact less the model when filling values\n",
    "* Will scale all the features because it's necessary to apply PCA\n",
    "* I am using std scaling because it keeps a realistic proportion of the data, without getting squashed by outliers like min-max.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4d825c-fd31-4183-a8fe-eb5d4244ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/kc_house_data.csv\", header=True, schema=schema, mode=\"PERMISSIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17653a42-557e-4505-ba45-7ced0ed341d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from house_price_predictor.features.pipeline import create_fitted_pipeline\n",
    "pipeline = create_fitted_pipeline(dataset=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44853cf9-d70b-4b76-8560-4026c53722f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "* Since the features are highly correlated with one another, we are using PCA to avoid further feature selection and speed-up model training. By suming the values of the vector of explained variances, I could observe that the first 13 Principal Componentes contained 95.7% of the explained variance. That's very reasonable, so it will be the number of K we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcaca347-42a0-4389-ae77-61ff20d3bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC 1: 0.31292743366538406\n",
      "PC 2: 0.4451170781945967\n",
      "PC 3: 0.5505288157224567\n",
      "PC 4: 0.6256772364262979\n",
      "PC 5: 0.6934923788496593\n",
      "PC 6: 0.7459239834052643\n",
      "PC 7: 0.7933722400006185\n",
      "PC 8: 0.8324624430810724\n",
      "PC 9: 0.868835054647886\n",
      "PC 10: 0.8965484493427118\n",
      "PC 11: 0.9198708947881981\n",
      "PC 12: 0.9412332098496586\n",
      "PC 13: 0.9570944891559053\n"
     ]
    }
   ],
   "source": [
    "pca = pipeline.stages[7]\n",
    "s = 0\n",
    "for idx, expl_var in enumerate(pca.explainedVariance):\n",
    "    s += expl_var\n",
    "    print(f\"PC {idx + 1}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73beab4-da83-4f74-8bc1-1e3d662f88d4",
   "metadata": {},
   "source": [
    "* This is an example of the final feature array of a single row (with which a model will be trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f0a1fc-af0a-4f63-b1e0-28170e3fbdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([2.6959, -0.3085, 0.3625, -0.7628, 0.351, 0.2867, -0.321, 0.9568, -0.0614, -0.1224, -1.2589, 0.0829, 0.1405])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.transform(df).select(\"features\").toPandas().iloc[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
